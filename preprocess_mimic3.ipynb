{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f64a7c4aa8c138f2a216c055f299fb9",
     "grade": false,
     "grade_id": "cell-757f7449dc485bc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Preprocessing of Procedure and Diagnoses codes:\n",
    "\n",
    "### Overview\n",
    "\n",
    "This program will preprocess the discharge summaries of the patients including the procudure and diagnoses codes available in the MIMIC_III dataset. After preparing the data, the word embeddings are then pre-trained using the Word2Vec method on all the discharge summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:33.533696Z",
     "start_time": "2022-02-01T22:26:33.525609Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17cd37dbc82c1a46157ec9adb81a3844",
     "grade": false,
     "grade_id": "cell-10db081f94b98d02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import csv\n",
    "import operator\n",
    "from utils.options import args\n",
    "from utils.utils import build_vocab, word_embeddings, fasttext_embeddings, gensim_to_fasttext_embeddings, \\\n",
    "    gensim_to_embeddings, \\\n",
    "    reformat, write_discharge_summaries, concat_data, split_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85cba0bf235d0a3f169d8bd9cf87350c",
     "grade": false,
     "grade_id": "cell-e59364a32d9b5fc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### About Raw Data\n",
    "\n",
    "For this paper, we will be using a clinical dataset from [MIMIC-III]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:33.662174Z",
     "start_time": "2022-02-01T22:26:33.536043Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fdd12a66d23c25566d9167b74f1f34e",
     "grade": false,
     "grade_id": "cell-d92e9dc996c61070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Y = 'full'\n",
    "notes_file = '%s/NOTEEVENTS.csv' % args.MIMIC_3_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cac1012cd2d5bacf94ba5393cf3ac3b",
     "grade": false,
     "grade_id": "cell-e092836975c6797a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1: Processing of the code-related files\n",
    "\n",
    "This step first involves reading of the procedure and diagnoses codes and then concatenating them to form an array. These are then copied to ALL_CODES.csv file to retrieve the unique ICD9 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:34.088515Z",
     "start_time": "2022-02-01T22:26:33.664247Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "dfproc = pd.read_csv('%s/PROCEDURES_ICD.csv' % args.MIMIC_3_DIR)\n",
    "dfdiag = pd.read_csv('%s/DIAGNOSES_ICD.csv' % args.MIMIC_3_DIR)\n",
    "\n",
    "dfdiag['absolute_code'] = dfdiag.apply(lambda row: str(reformat(str(row[4]), True)), axis=1)\n",
    "dfproc['absolute_code'] = dfproc.apply(lambda row: str(reformat(str(row[4]), False)), axis=1)\n",
    "\n",
    "dfcodes = pd.concat([dfdiag, dfproc])\n",
    "\n",
    "dfcodes.to_csv('%s/ALL_CODES.csv' % args.MIMIC_3_DIR, index=False,\n",
    "               columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'absolute_code'],\n",
    "               header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])\n",
    "\n",
    "df = pd.read_csv('%s/ALL_CODES.csv' % args.MIMIC_3_DIR, dtype={\"ICD9_CODE\": str})\n",
    "print(\"unique ICD9 code: {}\".format(len(df['ICD9_CODE'].unique())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d67dbea2960c373fa33683228f344817",
     "grade": false,
     "grade_id": "cell-c5413d29c37a0a33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 2: Process notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:35.354173Z",
     "start_time": "2022-02-01T22:26:35.349131Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "min_sentence_len = 3\n",
    "disch_full_file = write_discharge_summaries(\"%s/disch_full.csv\" % args.MIMIC_3_DIR, min_sentence_len,\n",
    "                                            '%s/NOTEEVENTS.csv' % (args.MIMIC_3_DIR))\n",
    "\n",
    "df = pd.read_csv('%s/disch_full.csv' % args.MIMIC_3_DIR)\n",
    "\n",
    "df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06d79d849cb089150d7cbf784274d7b0",
     "grade": false,
     "grade_id": "cell-4a3b0c07c74dc8dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3: Filter out the codes that not emerge in notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:35.646122Z",
     "start_time": "2022-02-01T22:26:35.641061Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "hadm_ids = set(df['HADM_ID'])\n",
    "with open('%s/ALL_CODES.csv' % args.MIMIC_3_DIR, 'r') as lf:\n",
    "    with open('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, 'w', newline='') as of:\n",
    "        w = csv.writer(of)\n",
    "        w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'])\n",
    "        r = csv.reader(lf)\n",
    "        # header\n",
    "        next(r)\n",
    "        for i, row in enumerate(r):\n",
    "            hadm_id = int(row[2])\n",
    "            # print(hadm_id)\n",
    "            # break\n",
    "            if hadm_id in hadm_ids:\n",
    "                w.writerow(row[1:3] + [row[-1], '', ''])\n",
    "\n",
    "dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, index_col=None)\n",
    "\n",
    "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "dfl.to_csv('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, index=False)\n",
    "\n",
    "sorted_file = '%s/disch_full.csv' % args.MIMIC_3_DIR\n",
    "df.to_csv(sorted_file, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a74bda5c299d66e1901820f18bb9aa25",
     "grade": false,
     "grade_id": "cell-845f881f710c6b3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4: Link notes with their code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:36.215845Z",
     "start_time": "2022-02-01T22:26:36.210288Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "labeled = concat_data('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, sorted_file,\n",
    "                      '%s/notes_labeled.csv' % args.MIMIC_3_DIR)\n",
    "\n",
    "dfnl = pd.read_csv(labeled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57aa824b8fd9cf75ae7da47f0ff3156b",
     "grade": false,
     "grade_id": "cell-8db3e08c181b90cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5: Statistic unique word, total word, HADM_ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:28:12.697366Z",
     "start_time": "2022-02-01T22:28:12.689275Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "types = set()\n",
    "num_tok = 0\n",
    "for row in dfnl.itertuples():\n",
    "    for w in row[3].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1\n",
    "\n",
    "print(\"num types\", len(types), \"num tokens\", num_tok)\n",
    "print(\"HADM_ID: {}\".format(len(dfnl['HADM_ID'].unique())))\n",
    "print(\"SUBJECT_ID: {}\".format(len(dfnl['SUBJECT_ID'].unique())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c28533d723309779b8530cb354815f28",
     "grade": false,
     "grade_id": "cell-a02bc53e8da63b0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 6: Split data into train dev test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:22:42.776691Z",
     "start_time": "2022-01-23T20:22:42.546473Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "034aed31d22576811319cd685d665149",
     "grade": true,
     "grade_id": "cell-c59bbb34681f832b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fname = '%s/notes_labeled.csv' % args.MIMIC_3_DIR\n",
    "base_name = \"%s/disch\" % args.MIMIC_3_DIR  # for output\n",
    "tr, dv, te = split_data(fname, base_name, args.MIMIC_3_DIR)\n",
    "\n",
    "vocab_min = 3\n",
    "vname = '%s/vocab.csv' % args.MIMIC_3_DIR\n",
    "build_vocab(vocab_min, tr, vname, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocab for RAC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:22:43.015001Z",
     "start_time": "2022-01-23T20:22:42.778233Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "vocab_min = 3\n",
    "vname = '%s/vocab_rac.csv' % args.MIMIC_3_DIR\n",
    "build_vocab(vocab_min, tr, vname, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f3c5b803ff31bc9fe6f02e659db1dc8",
     "grade": false,
     "grade_id": "cell-1a6ab34b3605550e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 7: Sort data by its note length, add length to the last column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:22:43.858476Z",
     "start_time": "2022-01-23T20:22:43.253148Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/disch_%s_split.csv' % (args.MIMIC_3_DIR, splt)\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_full.csv' % (args.MIMIC_3_DIR, splt), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train word embeddings via word2vec and fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = word_embeddings('full', '%s/disch_full.csv' % args.MIMIC_3_DIR, 100, 0, 5)\n",
    "gensim_to_embeddings('%s/processed_full_100.w2v' % args.MIMIC_3_DIR, '%s/vocab.csv' % args.MIMIC_3_DIR, Y)\n",
    "\n",
    "# generate word embeddings (300 dimensions) for convolved embedding model\n",
    "w2v_file = word_embeddings('full', '%s/disch_full.csv' % args.MIMIC_3_DIR, 300, 0, 5)\n",
    "gensim_to_embeddings('%s/processed_full_300.w2v' % args.MIMIC_3_DIR, '%s/vocab_rac.csv' % args.MIMIC_3_DIR, Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Statistic the top 50 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 50\n",
    "\n",
    "counts = Counter()\n",
    "dfnl = pd.read_csv('%s/notes_labeled.csv' % args.MIMIC_3_DIR)\n",
    "for row in dfnl.itertuples():\n",
    "    for label in str(row[4]).split(';'):\n",
    "        counts[label] += 1\n",
    "\n",
    "codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "codes_50 = [code[0] for code in codes_50[:Y]]\n",
    "\n",
    "with open('%s/TOP_%s_CODES.csv' % (args.MIMIC_3_DIR, str(Y)), 'w', newline='') as of:\n",
    "    w = csv.writer(of)\n",
    "    for code in codes_50:\n",
    "        w.writerow([code])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Split data according to train_50_hadm_ids dev... and test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    print(splt)\n",
    "    hadm_ids = set()\n",
    "    with open('%s/%s_50_hadm_ids.csv' % (args.MIMIC_3_DIR, splt), 'r') as f:\n",
    "        for line in f:\n",
    "            hadm_ids.add(line.rstrip())\n",
    "    with open('%s/notes_labeled.csv' % args.MIMIC_3_DIR, 'r') as f:\n",
    "        with open('%s/%s_%s.csv' % (args.MIMIC_3_DIR, splt, str(Y)), 'w', newline='') as of:\n",
    "            r = csv.reader(f)\n",
    "            w = csv.writer(of)\n",
    "            # header\n",
    "            w.writerow(next(r))\n",
    "            i = 0\n",
    "            for row in r:\n",
    "                hadm_id = row[1]\n",
    "                if hadm_id not in hadm_ids:\n",
    "                    continue\n",
    "                codes = set(str(row[3]).split(';'))\n",
    "                filtered_codes = codes.intersection(set(codes_50))\n",
    "                if len(filtered_codes) > 0:\n",
    "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
    "                    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Sort data by its note length, add length to the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/%s_%s.csv' % (args.MIMIC_3_DIR, splt, str(Y))\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_%s.csv' % (args.MIMIC_3_DIR, splt, str(Y)), index=False)"
   ]
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW1/HW1.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "832px",
    "left": "419px",
    "top": "110px",
    "width": "311.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
