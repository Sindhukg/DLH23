{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f64a7c4aa8c138f2a216c055f299fb9",
     "grade": false,
     "grade_id": "cell-757f7449dc485bc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Main.py\n",
    "\n",
    "## Overview\n",
    "\n",
    "The preprocessed data is fed to an encoder that will produce vector representations for words. These are later fed to a 'HierarchicalHyperbolic', one level at a time that will map ICD9 codes to these representaions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:33.533696Z",
     "start_time": "2022-02-01T22:26:33.525609Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17cd37dbc82c1a46157ec9adb81a3844",
     "grade": false,
     "grade_id": "cell-10db081f94b98d02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from gensim.models.poincare import PoincareModel\n",
    "\n",
    "from utils.utils import (\n",
    "    load_lookups,\n",
    "    prepare_instance,\n",
    "    MyDataset,\n",
    "    my_collate,\n",
    "    my_collate_longformer,\n",
    "    early_stop,\n",
    "    save_everything,\n",
    "    prepare_instance_longformer,\n",
    "    prepare_code_title\n",
    ")\n",
    "from utils.options import args\n",
    "from utils.models import pick_model\n",
    "from utils.train_test import train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:33.662174Z",
     "start_time": "2022-02-01T22:26:33.536043Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fdd12a66d23c25566d9167b74f1f34e",
     "grade": false,
     "grade_id": "cell-d92e9dc996c61070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "# decrease the maxInt value by factor 10 \n",
    "# as long as the OverflowError occurs.\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt / 10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46301c39a6360e38c7814cb9f4f1519a",
     "grade": false,
     "grade_id": "cell-d9212ce110d4dc0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load vocab and other lookups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:34.088515Z",
     "start_time": "2022-02-01T22:26:33.664247Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print(\"loading lookups...\")\n",
    "dicts = load_lookups(args) # load lookup table for tokens and icd codes\n",
    "\n",
    "if 'HierarchicalHyperbolic'.find(\"CodeTitle\") != -1:\n",
    "        dicts['c2title'] = prepare_code_title(dicts, args, 36)\n",
    "\n",
    "if 'HierarchicalHyperbolic'.find(\"Hyperbolic\") != -1:\n",
    "        print(\"Training hyperbolic embeddings...\")\n",
    "        hierarchy = dicts['hierarchy_dist']\n",
    "        # train poincare (hyperbolic) embeddings\n",
    "        relations = set()\n",
    "        for k, v in hierarchy[4].items():\n",
    "            relations.add(('root', v[0]))\n",
    "            for i in range(4):\n",
    "                relations.add(tuple(v[i:i+2]))\n",
    "        relations = list(relations)\n",
    "        poincare = PoincareModel(relations, 50, negative=10)\n",
    "        poincare.train(epochs=50)\n",
    "        dicts['poincare_embeddings'] = poincare.kv\n",
    "    \n",
    "if 'HierarchicalHyperbolic' == \"CodeTitle\" or 'HierarchicalHyperbolic' == \"RandomlyInitialized\" or 'HierarchicalHyperbolic' == \"LAAT'HierarchicalHyperbolic'\":\n",
    "        depth = 1\n",
    "\n",
    "model = pick_model(args, dicts)\n",
    "print(model)\n",
    "    \n",
    "if not 'None':\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=0, lr=1e-4)\n",
    "else:\n",
    "        optimizer = None\n",
    "\n",
    "if model.find(\"longformer\") != -1:\n",
    "        prepare_instance_func = prepare_instance_longformer\n",
    "else:\n",
    "        prepare_instance_func = prepare_instance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing model instances and optimizer for Longformer and LAAT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:34.576740Z",
     "start_time": "2022-02-01T22:26:34.091155Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e7427796cf3b4d5a493bd096d082ac9",
     "grade": false,
     "grade_id": "cell-d28045e592b8f081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = './data/mimic3/train_50.csv'\n",
    "train_instances = prepare_instance_func(dicts, data_path, args, 4096)\n",
    "print(\"train_instances {}\".format(len(train_instances)))\n",
    "if 'mimic3' != 'mimic2':\n",
    "        dev_instances = prepare_instance_func(dicts, data_path.replace('train','dev'), args, 4096)\n",
    "        print(\"dev_instances {}\".format(len(dev_instances)))\n",
    "else:\n",
    "        dev_instances = None\n",
    "test_instances = prepare_instance_func(dicts, data_path.replace('train','test'), args, 4096)\n",
    "print(\"test_instances {}\".format(len(test_instances)))\n",
    "\n",
    "if model.find(\"longformer\") != -1:\n",
    "        collate_func = my_collate_longformer\n",
    "else:\n",
    "        collate_func = my_collate\n",
    "\n",
    "train_loader = DataLoader(MyDataset(train_instances), 16, shuffle=True, collate_fn=collate_func, num_workers=16, pin_memory=True)\n",
    "if 'mimic3' != 'mimic2':\n",
    "        dev_loader = DataLoader(MyDataset(dev_instances), 1, shuffle=False, collate_fn=collate_func, num_workers=16, pin_memory=True)\n",
    "else:\n",
    "        dev_loader = None\n",
    "test_loader = DataLoader(MyDataset(test_instances), 1, shuffle=False, collate_fn=collate_func, num_workers=16, pin_memory=True)\n",
    "\n",
    "scheduler = None\n",
    "if model.find(\"LAAT\") != -1 and not 'None':\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=scheduler)\n",
    "    \n",
    "if not 'None' and model.find(\"longformer\") != -1:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "test_only = 'None' is not None\n",
    "\n",
    "start_depth = 5 - depth\n",
    "cur_depth = 4 if test_only else start_depth\n",
    "\n",
    "epochs = [int(epoch) for epoch in \"2,3,5,10,500\".split(',')]\n",
    "print(f\"Total epochs at each level: {epochs}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model at depth < 5.\n",
    "\n",
    "This paper involves training the model in 5 levels - level 0 through 4. Each level has different number of epochs that can be modified according to our needs and this will have an impact on the run time and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:34.861688Z",
     "start_time": "2022-02-01T22:26:34.578395Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d140e954c3a28d1e5479935f3ef713e",
     "grade": true,
     "grade_id": "cell-8f9c85ba731d96e5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "while cur_depth < 5:\n",
    "        metrics_hist = defaultdict(lambda: [])\n",
    "        metrics_hist_te = defaultdict(lambda: [])\n",
    "        metrics_hist_tr = defaultdict(lambda: [])\n",
    "        break_loop = False\n",
    "        if not test_only:\n",
    "            print(\"Training model at depth {}:\".format(cur_depth))\n",
    "            if cur_depth != 0:\n",
    "                if isinstance(model, torch.nn.DataParallel):\n",
    "                    model.module.decoder.change_depth(cur_depth)\n",
    "                else:\n",
    "                    model.decoder.change_depth(cur_depth)\n",
    "        for epoch in range(epochs[cur_depth]):\n",
    "            if epoch == 0 and cur_depth == start_depth and not test_model:\n",
    "                model_dir = os.path.join('./models', '_'.join([model, 'HierarchicalHyperbolic', time.strftime('%b_%d_%H_%M_%S', time.localtime())]))\n",
    "                os.makedirs(model_dir)\n",
    "            elif test_model:\n",
    "                model_dir = os.path.dirname(os.path.abspath(test_model))\n",
    "\n",
    "            if not test_only and not break_loop:\n",
    "                epoch_start = time.time()\n",
    "                gpu_list = [int(idx) for idx in args.gpu.split(',')]\n",
    "                losses = train(args, model, optimizer, scheduler, epoch, gpu_list, train_loader, cur_depth)\n",
    "                loss = np.mean(losses)\n",
    "                epoch_finish = time.time()\n",
    "                print(\"epoch finish in %.2fs, loss: %.4f\" % (epoch_finish - epoch_start, loss))\n",
    "            else:\n",
    "                loss = np.nan\n",
    "\n",
    "            fold = 'test' if 'mimic3' == 'mimic2' else 'dev'\n",
    "            dev_instances = test_instances if 'mimic3' == 'mimic2' else dev_instances\n",
    "            dev_loader = test_loader if 'mimic3' == 'mimic2' else dev_loader\n",
    "            if epoch == epochs[cur_depth] - 1:\n",
    "                print(\"last epoch: testing on dev and test sets\")\n",
    "                break_loop = True\n",
    "            \n",
    "            # test on dev\n",
    "            evaluation_start = time.time()\n",
    "            metrics = test(args, model, data_path, fold, gpu_list, dicts, dev_loader, cur_depth)\n",
    "            evaluation_finish = time.time()\n",
    "            print(\"evaluation finish in %.2fs\" % (evaluation_finish - evaluation_start))\n",
    "            if test_only or break_loop or epoch == epochs[cur_depth] - 1:\n",
    "                metrics_te = test(args, model, data_path, \"test\", gpu_list, dicts, test_loader, cur_depth)\n",
    "            else:\n",
    "                metrics_te = defaultdict(float)\n",
    "            metrics_tr = {'loss': loss}\n",
    "            metrics_all = (metrics, metrics_te, metrics_tr)\n",
    "\n",
    "            for name in metrics_all[0].keys():\n",
    "                metrics_hist[name].append(metrics_all[0][name])\n",
    "            for name in metrics_all[1].keys():\n",
    "                metrics_hist_te[name].append(metrics_all[1][name])\n",
    "            for name in metrics_all[2].keys():\n",
    "                metrics_hist_tr[name].append(metrics_all[2][name])\n",
    "            metrics_hist_all = (metrics_hist, metrics_hist_te, metrics_hist_tr)\n",
    "\n",
    "            save_everything(args, metrics_hist_all, model, model_dir, None, 'prec_at_8', test_only)\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if test_only or break_loop:\n",
    "                break\n",
    "\n",
    "            if 'prec_at_8' in metrics_hist.keys():\n",
    "                if early_stop(metrics_hist, 'prec_at_8', 10):\n",
    "                    #stop training, do tests on test and train sets, and then stop the script\n",
    "                    print(\"%s hasn't improved in %d epochs, early stopping...\" % ('prec_at_8', 10))\n",
    "                    break_loop = True\n",
    "                    test_model = '%s/model_best_%s.pth' % (model_dir, 'prec_at_8')\n",
    "                    tmp = depth\n",
    "                    depth = 5 - cur_depth\n",
    "                    model = pick_model(args, dicts)\n",
    "                    depth = tmp\n",
    "\n",
    "            if scheduler is not None and 'prec_at_8' in metrics_hist.keys():\n",
    "                if early_stop(metrics_hist, 'prec_at_8', 5):\n",
    "                    scheduler.step()\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        print(f\"{'prec_at_8'} hasn't improved in {5} epochs, reduce learning rate to {param_group['lr']}\")\n",
    "\n",
    "        cur_depth += 1\n"
   ]
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW1/HW1.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "832px",
    "left": "419px",
    "top": "110px",
    "width": "311.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
