{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f64a7c4aa8c138f2a216c055f299fb9",
     "grade": false,
     "grade_id": "cell-757f7449dc485bc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Encoder-Decoder Models:\n",
    "\n",
    "## Overview\n",
    "\n",
    " This program has the encoder-decoder model details and we can change the model option as per the requirements. This paper uses three different types of encoder architectures namely: BiLSTM, MultiResCNN, and RAC Reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:33.533696Z",
     "start_time": "2022-02-01T22:26:33.525609Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17cd37dbc82c1a46157ec9adb81a3844",
     "grade": false,
     "grade_id": "cell-10db081f94b98d02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_ as xavier_uniform\n",
    "import numpy as np\n",
    "from utils.utils import build_pretrain_embedding, load_embeddings\n",
    "from utils.losses import AsymmetricLoss, AsymmetricLossOptimized\n",
    "from math import floor, sqrt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:33.662174Z",
     "start_time": "2022-02-01T22:26:33.536043Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fdd12a66d23c25566d9167b74f1f34e",
     "grade": false,
     "grade_id": "cell-d92e9dc996c61070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class WordRep(nn.Module):\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(WordRep, self).__init__()\n",
    "\n",
    "        if args.embed_file:\n",
    "            print(\"loading pretrained embeddings from {}\".format(args.embed_file))\n",
    "            if args.use_ext_emb:\n",
    "                pretrain_word_embedding, pretrain_emb_dim = build_pretrain_embedding(args.embed_file, dicts['w2ind'],\n",
    "                                                                                     True)\n",
    "                W = torch.from_numpy(pretrain_word_embedding)\n",
    "            else:\n",
    "                W = torch.Tensor(load_embeddings(args.embed_file))\n",
    "\n",
    "            self.embed = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)\n",
    "            self.embed.weight.data = W.clone()\n",
    "        else:\n",
    "            # add 2 to include UNK and PAD\n",
    "            self.embed = nn.Embedding(len(dicts['w2ind']) + 2, args.embed_size, padding_idx=0)\n",
    "        self.feature_size = self.embed.embedding_dim\n",
    "\n",
    "        self.embed_drop = nn.Dropout(p=args.dropout)\n",
    "\n",
    "        self.conv_dict = {1: [self.feature_size, args.num_filter_maps],\n",
    "                     2: [self.feature_size, 100, args.num_filter_maps],\n",
    "                     3: [self.feature_size, 150, 100, args.num_filter_maps],\n",
    "                     4: [self.feature_size, 200, 150, 100, args.num_filter_maps]\n",
    "                     }\n",
    "        \n",
    "        def forward(self, x):\n",
    "        features = [self.embed(x)]\n",
    "\n",
    "        x = torch.cat(features, dim=2)\n",
    "\n",
    "        x = self.embed_drop(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization of the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:34.088515Z",
     "start_time": "2022-02-01T22:26:33.664247Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class RandomlyInitializedDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The original per-label attention network: query is randomly initialized\n",
    "    \"\"\"\n",
    "    def __init__(self, args, Y, dicts, input_size):\n",
    "        super(RandomlyInitializedDecoder, self).__init__()\n",
    "\n",
    "        Y = Y[-1]\n",
    "\n",
    "        self.U = nn.Linear(input_size, Y)\n",
    "        xavier_uniform(self.U.weight)\n",
    "\n",
    "\n",
    "        self.final = nn.Linear(input_size, Y)\n",
    "        xavier_uniform(self.final.weight)\n",
    "\n",
    "        self.loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    def forward(self, x, target, text_inputs):\n",
    "        # attention\n",
    "        alpha = F.softmax(self.U.weight.matmul(x.transpose(1, 2)), dim=2)\n",
    "\n",
    "        m = alpha.matmul(x)\n",
    "\n",
    "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
    "\n",
    "        loss = self.loss_function(y, target)\n",
    "        return y, loss, alpha, m\n",
    "    \n",
    "    def change_depth(self, depth=0):\n",
    "        # placeholder\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RACDecoder - The decoder proposed by Kim et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:34.576740Z",
     "start_time": "2022-02-01T22:26:34.091155Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e7427796cf3b4d5a493bd096d082ac9",
     "grade": false,
     "grade_id": "cell-d28045e592b8f081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RACDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder proposed by Kim et al. (Code title-guided attention)\n",
    "    \"\"\"\n",
    "    def __init__(self, args, Y, dicts, input_size):\n",
    "        super(RACDecoder, self).__init__()\n",
    "\n",
    "        Y = Y[-1]\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.register_buffer(\"c2title\", torch.LongTensor(dicts[\"c2title\"]))\n",
    "        self.word_rep = WordRep(args, Y, dicts)\n",
    "\n",
    "        filter_size = int(args.code_title_filter_size)\n",
    "        self.code_title_conv = nn.Conv1d(self.word_rep.feature_size, input_size,\n",
    "                                         filter_size, padding=int(floor(filter_size / 2)))\n",
    "        xavier_uniform(self.code_title_conv.weight)\n",
    "        self.code_title_maxpool = nn.MaxPool1d(args.num_code_title_tokens)\n",
    "\n",
    "        self.final = nn.Linear(input_size, Y)\n",
    "        xavier_uniform(self.final.weight)\n",
    "\n",
    "        self.loss_function = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, x, target, text_inputs):\n",
    "        code_title = self.word_rep(self._buffers['c2title']).transpose(1, 2)\n",
    "        # attention\n",
    "        U = self.code_title_conv(code_title)\n",
    "        U = self.code_title_maxpool(U).squeeze(-1)\n",
    "        U = torch.tanh(U)\n",
    "\n",
    "        attention_score = U.matmul(x.transpose(1, 2)) / sqrt(self.input_size)\n",
    "        alpha = F.softmax(attention_score, dim=2)\n",
    "\n",
    "        m = alpha.matmul(x)\n",
    "\n",
    "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
    "\n",
    "        loss = self.loss_function(y, target)\n",
    "        return y, loss, alpha, m\n",
    "\n",
    "    def change_depth(self, depth=0):\n",
    "        # placeholder\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAATDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:34.861688Z",
     "start_time": "2022-02-01T22:26:34.578395Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d140e954c3a28d1e5479935f3ef713e",
     "grade": true,
     "grade_id": "cell-8f9c85ba731d96e5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LAATDecoder(nn.Module):\n",
    "    def __init__(self, args, Y, dicts, input_size):\n",
    "        super(LAATDecoder, self).__init__()\n",
    "\n",
    "        Y = Y[-1]\n",
    "\n",
    "        self.attn_dim = args.attn_dim\n",
    "        self.W = nn.Linear(input_size, self.attn_dim)\n",
    "        self.U = nn.Linear(self.attn_dim, Y)\n",
    "        xavier_uniform(self.W.weight)\n",
    "        xavier_uniform(self.U.weight)\n",
    "\n",
    "        self.final = nn.Linear(input_size, Y)\n",
    "        xavier_uniform(self.final.weight)\n",
    "\n",
    "        self.loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x, target, text_inputs):\n",
    "        z = torch.tanh(self.W(x))\n",
    "        # attention\n",
    "        alpha = F.softmax(self.U.weight.matmul(z.transpose(1, 2)), dim=2)\n",
    "\n",
    "        m = alpha.matmul(x)\n",
    "\n",
    "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
    "\n",
    "        loss = self.loss_function(y, target)\n",
    "        return y, loss, alpha, m\n",
    "\n",
    "    def change_depth(self, depth=0):\n",
    "        # placeholder\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge transfer initialization and hyperbolic embedding correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:35.347882Z",
     "start_time": "2022-02-01T22:26:34.863302Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "557341b6198f8525680acb1753cbbdbf",
     "grade": true,
     "grade_id": "cell-343b745247b90367",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder: knowledge transfer initialization and hyperbolic embedding correction\n",
    "    \"\"\"\n",
    "    def __init__(self, args, Y, dicts, input_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dicts = dicts\n",
    "\n",
    "        self.decoder_dict = nn.ModuleDict()\n",
    "        for i in range(len(Y)):\n",
    "            y = Y[i]\n",
    "            self.decoder_dict[str(i) + '_' + '0'] = nn.Linear(input_size, y)\n",
    "            self.decoder_dict[str(i) + '_' + '1'] = nn.Linear(input_size, y)\n",
    "            xavier_uniform(self.decoder_dict[str(i) + '_' + '0'].weight)\n",
    "            xavier_uniform(self.decoder_dict[str(i) + '_' + '1'].weight)\n",
    "        \n",
    "        self.use_hyperbolic =  args.decoder.find(\"Hyperbolic\") != -1\n",
    "        if self.use_hyperbolic:\n",
    "            self.cat_hyperbolic = args.cat_hyperbolic\n",
    "            if not self.cat_hyperbolic:\n",
    "                self.hyperbolic_fc_dict = nn.ModuleDict()\n",
    "                for i in range(len(Y)):\n",
    "                    self.hyperbolic_fc_dict[str(i)] = nn.Linear(args.hyperbolic_dim, input_size)\n",
    "            else:\n",
    "                self.query_fc_dict = nn.ModuleDict()\n",
    "                for i in range(len(Y)):\n",
    "                    self.query_fc_dict[str(i)] = nn.Linear(input_size + args.hyperbolic_dim, input_size)\n",
    "            \n",
    "            # build hyperbolic embedding matrix\n",
    "            self.hyperbolic_emb_dict = {}\n",
    "            for i in range(len(Y)):\n",
    "                self.hyperbolic_emb_dict[i] = np.zeros((Y[i], args.hyperbolic_dim))\n",
    "                for idx, code in dicts['ind2c'][i].items():\n",
    "                    self.hyperbolic_emb_dict[i][idx, :] = np.copy(dicts['poincare_embeddings'].get_vector(code))\n",
    "                self.register_buffer(name='hb_emb_' + str(i), tensor=torch.tensor(self.hyperbolic_emb_dict[i], dtype=torch.float32))\n",
    "\n",
    "        self.cur_depth = 5 - args.depth\n",
    "        self.is_init = False\n",
    "        self.change_depth(self.cur_depth)\n",
    "\n",
    "        if args.loss == 'BCE':\n",
    "            self.loss_function = nn.BCEWithLogitsLoss()\n",
    "        elif args.loss == 'ASL':\n",
    "            asl_config = [float(c) for c in args.asl_config.split(',')]\n",
    "            self.loss_function = AsymmetricLoss(gamma_neg=asl_config[0], gamma_pos=asl_config[1],\n",
    "                                                clip=asl_config[2], reduction=args.asl_reduction)\n",
    "        elif args.loss == 'ASLO':\n",
    "            asl_config = [float(c) for c in args.asl_config.split(',')]\n",
    "            self.loss_function = AsymmetricLossOptimized(gamma_neg=asl_config[0], gamma_pos=asl_config[1],\n",
    "                                                         clip=asl_config[2], reduction=args.asl_reduction)\n",
    "    \n",
    "    def change_depth(self, depth=0):\n",
    "        if self.is_init:\n",
    "            # copy previous attention weights to current attention network based on ICD hierarchy\n",
    "            ind2c = self.dicts['ind2c']\n",
    "            c2ind = self.dicts['c2ind']\n",
    "            hierarchy_dist = self.dicts['hierarchy_dist']\n",
    "            for i, code in ind2c[depth].items():\n",
    "                tree = hierarchy_dist[depth][code]\n",
    "                pre_idx = c2ind[depth - 1][tree[depth - 1]]\n",
    "\n",
    "                self.decoder_dict[str(depth) + '_' + '0'].weight.data[i, :] = self.decoder_dict[str(depth - 1) + '_' + '0'].weight.data[pre_idx, :].clone()\n",
    "                self.decoder_dict[str(depth) + '_' + '1'].weight.data[i, :] = self.decoder_dict[str(depth - 1) + '_' + '1'].weight.data[pre_idx, :].clone()\n",
    "\n",
    "        if not self.is_init:\n",
    "            self.is_init = True\n",
    "\n",
    "        self.cur_depth = depth\n",
    "        \n",
    "    def forward(self, x, target, text_inputs):\n",
    "        # attention\n",
    "        if self.use_hyperbolic:\n",
    "            if not self.cat_hyperbolic:\n",
    "                query = self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight + self.hyperbolic_fc_dict[str(self.cur_depth)](self._buffers['hb_emb_' + str(self.cur_depth)])\n",
    "            else:\n",
    "                query = torch.cat([self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight, self._buffers['hb_emb_' + str(self.cur_depth)]], dim=1)\n",
    "                query = self.query_fc_dict[str(self.cur_depth)](query)\n",
    "        else:\n",
    "            query = self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight\n",
    "\n",
    "        alpha = F.softmax(query.matmul(x.transpose(1, 2)), dim=2)\n",
    "        m = alpha.matmul(x)\n",
    "\n",
    "        y = self.decoder_dict[str(self.cur_depth) + '_' + '1'].weight.mul(m).sum(dim=2).add(self.decoder_dict[str(self.cur_depth) + '_' + '1'].bias)\n",
    "\n",
    "        loss = self.loss_function(y, target)\n",
    "        \n",
    "        return y, loss, alpha, m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:35.354173Z",
     "start_time": "2022-02-01T22:26:35.349131Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, kernel_size, stride, use_res, dropout):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv1d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=int(floor(kernel_size / 2)), bias=False),\n",
    "            nn.BatchNorm1d(outchannel),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(outchannel, outchannel, kernel_size=kernel_size, stride=1, padding=int(floor(kernel_size / 2)), bias=False),\n",
    "            nn.BatchNorm1d(outchannel)\n",
    "        )\n",
    "\n",
    "        self.use_res = use_res\n",
    "        if self.use_res:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                        nn.Conv1d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                        nn.BatchNorm1d(outchannel)\n",
    "                    )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        if self.use_res:\n",
    "            out += self.shortcut(x)\n",
    "        out = torch.tanh(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder model - MultiResCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:35.639049Z",
     "start_time": "2022-02-01T22:26:35.355685Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12811f08d77cb33b2b8821e3da47d7c9",
     "grade": true,
     "grade_id": "cell-7207b1c9d6f08a03",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MultiResCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(MultiResCNN, self).__init__()\n",
    "\n",
    "        self.word_rep = WordRep(args, Y, dicts)\n",
    "\n",
    "        self.conv = nn.ModuleList()\n",
    "        filter_sizes = args.filter_size.split(',')\n",
    "\n",
    "        self.filter_num = len(filter_sizes)\n",
    "        for filter_size in filter_sizes:\n",
    "            filter_size = int(filter_size)\n",
    "            one_channel = nn.ModuleList()\n",
    "            tmp = nn.Conv1d(self.word_rep.feature_size, self.word_rep.feature_size, kernel_size=filter_size,\n",
    "                            padding=int(floor(filter_size / 2)))\n",
    "            xavier_uniform(tmp.weight)\n",
    "            one_channel.add_module('baseconv', tmp)\n",
    "\n",
    "            conv_dimension = self.word_rep.conv_dict[args.conv_layer]\n",
    "            for idx in range(args.conv_layer):\n",
    "                tmp = ResidualBlock(conv_dimension[idx], conv_dimension[idx + 1], filter_size, 1, True,\n",
    "                                    args.dropout)\n",
    "                one_channel.add_module('resconv-{}'.format(idx), tmp)\n",
    "\n",
    "            self.conv.add_module('channel-{}'.format(filter_size), one_channel)\n",
    "\n",
    "        if args.decoder == \"HierarchicalHyperbolic\" or args.decoder == \"Hierarchical\":\n",
    "            self.decoder = Decoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
    "        elif args.decoder == \"RandomlyInitialized\":\n",
    "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
    "        elif args.decoder == \"CodeTitle\":\n",
    "            self.decoder = RACDecoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
    "        else:\n",
    "            raise RuntimeError(\"wrong decoder name\")\n",
    "\n",
    "        self.cur_depth = 5 - args.depth\n",
    "\n",
    "\n",
    "    def forward(self, x, target, text_inputs):\n",
    "        x = self.word_rep(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        conv_result = []\n",
    "        for conv in self.conv:\n",
    "            tmp = x\n",
    "            for idx, md in enumerate(conv):\n",
    "                if idx == 0:\n",
    "                    tmp = torch.tanh(md(tmp))\n",
    "                else:\n",
    "                    tmp = md(tmp)\n",
    "            tmp = tmp.transpose(1, 2)\n",
    "            conv_result.append(tmp)\n",
    "        x = torch.cat(conv_result, dim=2)\n",
    "\n",
    "        y, loss, alpha, m = self.decoder(x, target, text_inputs)\n",
    "\n",
    "        return y, loss, alpha, m\n",
    "\n",
    "    def freeze_net(self):\n",
    "        for p in self.word_rep.embed.parameters():\n",
    "            p.requires_grad = False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder model - RACReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:36.208414Z",
     "start_time": "2022-02-01T22:26:35.649618Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a72ba59ec4465e07159f63371dcaaeca",
     "grade": true,
     "grade_id": "cell-a298eb0fe4ed28f9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class RACReader(nn.Module):\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(RACReader, self).__init__()\n",
    "\n",
    "        self.word_rep = WordRep(args, Y, dicts)\n",
    "        filter_size = int(args.filter_size)\n",
    "\n",
    "        self.conv = nn.ModuleList()\n",
    "        for i in range(args.reader_conv_num):\n",
    "            conv = nn.Conv1d(self.word_rep.feature_size, self.word_rep.feature_size, kernel_size=filter_size,\n",
    "                                padding=int(floor(filter_size / 2)))\n",
    "            xavier_uniform(conv.weight)\n",
    "            self.conv.add_module(f'conv_{i+1}', conv)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=args.dropout)\n",
    "\n",
    "        self.trans = nn.ModuleList()\n",
    "        for i in range(args.reader_trans_num):\n",
    "            trans = nn.TransformerEncoderLayer(self.word_rep.feature_size, 1, args.trans_ff_dim, args.dropout, \"relu\")\n",
    "            self.trans.add_module(f'trans_{i+1}', trans)\n",
    "\n",
    "        if args.decoder == \"HierarchicalHyperbolic\" or args.decoder == \"Hierarchical\":\n",
    "            self.decoder = Decoder(args, Y, dicts, self.word_rep.feature_size)\n",
    "        elif args.decoder == \"RandomlyInitialized\":\n",
    "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.word_rep.feature_size)\n",
    "        elif args.decoder == \"CodeTitle\":\n",
    "            self.decoder = RACDecoder(args, Y, dicts, self.word_rep.feature_size)\n",
    "        else:\n",
    "            raise RuntimeError(\"wrong decoder name\")\n",
    "    \n",
    "    def forward(self, x, target, text_inputs=None):\n",
    "        x = self.word_rep(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for conv in self.conv:\n",
    "            x = conv(x)\n",
    "\n",
    "        x = torch.tanh(x).permute(2, 0, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for trans in self.trans:\n",
    "            x = trans(x)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        y, loss, alpha, m = self.decoder(x, target, text_inputs)\n",
    "\n",
    "        return y, loss, alpha, m\n",
    "    \n",
    "    def freeze_net(self):\n",
    "        for p in self.word_rep.embed.parameters():\n",
    "            p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder model - LAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:36.215845Z",
     "start_time": "2022-02-01T22:26:36.210288Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "class LAAT(nn.Module):\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(LAAT, self).__init__()\n",
    "        self.word_rep = WordRep(args, Y, dicts)\n",
    "\n",
    "        self.hidden_dim = args.lstm_hidden_dim\n",
    "        self.biLSTM = nn.LSTM(input_size=self.word_rep.feature_size,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=args.dropout,\n",
    "            bidirectional=False\n",
    "            )\n",
    "\n",
    "        self.output_dim = 2 * self.hidden_dim\n",
    "        self.use_LAAT = False\n",
    "\n",
    "        self.attn_dim = args.attn_dim\n",
    "        self.decoder_name = args.decoder\n",
    "        if \"LAAT\" in args.decoder:\n",
    "            if args.decoder == \"LAATHierarchicalHyperbolic\" or args.decoder == \"LAATHierarchical\":\n",
    "                self.decoder_name = args.decoder[4:]\n",
    "            self.output_dim = self.attn_dim\n",
    "            self.use_LAAT = True\n",
    "            self.W = nn.Linear(2 * self.hidden_dim, self.attn_dim)\n",
    "\n",
    "        if self.decoder_name == \"HierarchicalHyperbolic\" or self.decoder_name == \"Hierarchical\":\n",
    "            self.decoder = Decoder(args, Y, dicts, self.output_dim)\n",
    "        elif self.decoder_name == \"RandomlyInitialized\":\n",
    "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.output_dim)\n",
    "        elif self.decoder_name == \"CodeTitle\":\n",
    "            self.decoder = RACDecoder(args, Y, dicts, self.output_dim)\n",
    "        elif self.decoder_name == \"LAATDecoder\":\n",
    "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.output_dim)\n",
    "        else:\n",
    "            raise RuntimeError(\"wrong decoder name\")\n",
    "\n",
    "        \n",
    "\n",
    "        self.cur_depth = 5 - args.depth\n",
    "\n",
    "    def forward(self, x, target, text_inputs):\n",
    "        # lengths = (x > 0).sum(dim=1).cpu()\n",
    "        x = self.word_rep(x)  # [batch, length, input_size]\n",
    "\n",
    "        # x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        x1 = self.biLSTM(x)[0]\n",
    "        # x1 = pad_packed_sequence(x1, batch_first=True)[0]\n",
    "\n",
    "        if self.use_LAAT:\n",
    "            x1 = torch.tanh(self.W(x1))\n",
    "\n",
    "        y, loss, alpha, m = self.decoder(x1, target, text_inputs)\n",
    "\n",
    "        return y, loss, alpha, m\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:26:36.939666Z",
     "start_time": "2022-02-01T22:26:36.218546Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f36119f0574d09e596cc891bd1f71bd4",
     "grade": true,
     "grade_id": "cell-b5931f772d6dac9f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pick_model(args, dicts):\n",
    "    ind2c = dicts['ind2c']\n",
    "    Y = [len(ind2c[i]) for i in range(5)] # total number of ICD codes\n",
    "    if args.model == 'MultiResCNN':\n",
    "        model = MultiResCNN(args, Y, dicts)\n",
    "    elif args.model == 'longformer':\n",
    "        model = LongformerClassifier(args, Y, dicts)\n",
    "    elif args.model == 'RACReader':\n",
    "        model = RACReader(args, Y, dicts)\n",
    "    elif args.model == 'LAAT':\n",
    "        model = LAAT(args, Y, dicts)\n",
    "    else:\n",
    "        raise RuntimeError(\"wrong model name\")\n",
    "\n",
    "    if args.test_model:\n",
    "        model.decoder.change_depth(4)\n",
    "        sd = torch.load(args.test_model)\n",
    "        model.load_state_dict(sd)\n",
    "    if args.tune_wordemb == False:\n",
    "        model.freeze_net()\n",
    "    #if len(args.gpu_list) == 1 and args.gpu_list[0] != -1: # single card training\n",
    "    #   model.cuda()\n",
    "    elif len(args.gpu_list) > 1: # multi-card training\n",
    "        model = nn.DataParallel(model, device_ids=args.gpu_list)\n",
    "    #    model = model.to(f'cuda:{model.device_ids[0]}')\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW1/HW1.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "832px",
    "left": "419px",
    "top": "110px",
    "width": "311.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
